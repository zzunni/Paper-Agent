

===== Page 1 =====

Defect Detection on Semiconductor Wafers by
Distribution Analysis
Thomas Olschewski∗, Technische Universit¨at Dresden
November 9, 2021
Abstract
A method for object classiﬁcation that is based on distribution analysis is proposed. In addition, a method for ﬁnding relevant features and
the uniﬁcation of this algorithm with another classiﬁcation algorithm is
proposed. The presented classiﬁcation algorithm has been applied successfully to real-world measurement data from wafer fabrication of close
to hundred thousand chips of several product types. The presented algorithm prefers ﬁnding the best rater in a low-dimensional search space over
ﬁnding a good rater in a high-dimensional search space. Our approach is
interesting in that it is fast (quasi-linear) and reached good to excellent
prediction or detection quality for real-world wafer data.
∗thomas.olschewski@tu-dresden.de
1
arXiv:2111.03727v1  [cs.LG]  5 Nov 2021

===== Page 2 =====

Contents
1
Acknowledgements
3
2
Introduction
3
2.1
Overview
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
4
3
The Algorithms
6
3.1
Deﬁnitions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
6
3.1.1
Scaling
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
6
3.1.2
Histogram Functions . . . . . . . . . . . . . . . . . . . . .
6
3.1.3
Finding Candidate Indicator Columns . . . . . . . . . . .
9
3.1.4
Computing Indicator Values SC(i)
. . . . . . . . . . . . .
9
3.1.5
Computing Predictions F(i, c)
. . . . . . . . . . . . . . .
10
3.1.6
CutoﬀSelection . . . . . . . . . . . . . . . . . . . . . . . .
10
3.1.7
Classiﬁcation Algorithm . . . . . . . . . . . . . . . . . . .
12
3.2
Free Parameters: b+, b−And nb
. . . . . . . . . . . . . . . . . .
12
3.3
Considerations on µj And σj
. . . . . . . . . . . . . . . . . . . .
13
3.4
Limits . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
14
4
Experimental Setup
15
4.1
Task And Data . . . . . . . . . . . . . . . . . . . . . . . . . . . .
15
4.2
The Data Basis . . . . . . . . . . . . . . . . . . . . . . . . . . . .
16
4.3
The Implementation . . . . . . . . . . . . . . . . . . . . . . . . .
16
5
Results
17
5.1
Properties . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
17
5.2
Notational Remark . . . . . . . . . . . . . . . . . . . . . . . . . .
17
5.3
Predicting or Detecting Frontend Defect Status of Chips . . . . .
17
5.3.1
Predicting Frontend Defect Status S2 of Chips
. . . . . .
17
5.3.2
Detecting Frontend Defect Status S1 of Chips . . . . . . .
20
5.3.3
Predicting Frontend Defect States Knowing Only Part of
Measurement Data . . . . . . . . . . . . . . . . . . . . . .
22
5.4
Predicting Backend Defect Status of Chips . . . . . . . . . . . . .
24
5.4.1
Predicting Backend Defect Status of Chips Knowing Frontend Data . . . . . . . . . . . . . . . . . . . . . . . . . . .
25
5.4.2
Predicting Backend Defect States Knowing Only Part of
Measurement Data . . . . . . . . . . . . . . . . . . . . . .
25
5.4.3
Addendum: Batch-Wise CutoﬀOptimization . . . . . . .
26
5.5
Classifying Iris Types
. . . . . . . . . . . . . . . . . . . . . . . .
28
5.6
Dimensional Reduction . . . . . . . . . . . . . . . . . . . . . . . .
28
5.7
Automatizing The Finding of Cics
. . . . . . . . . . . . . . . . .
30
5.7.1
Practical Results by Using AutoCics() . . . . . . . . . . .
30
5.7.2
Obsoleting Free Parameters by Using AutoCics() . . . . .
31
1

===== Page 3 =====

6
Algorithmic Complexity
32
6.1
Scaling (Algorithm 1)
. . . . . . . . . . . . . . . . . . . . . . . .
32
6.2
Computing one histogram Ha,nb(x1, . . . , x ¯m) . . . . . . . . . . . .
32
6.3
Computing FindCics(Scale(X), T +, T −, b+, b−, nb) (Algorithm 2)
33
6.4
Computing SC(I¬T ) (Algorithm 3) . . . . . . . . . . . . . . . . .
33
6.5
Computing predictions F(i, c) (Algorithm 4)
. . . . . . . . . . .
33
6.6
Na¨ıve cutoﬀselection . . . . . . . . . . . . . . . . . . . . . . . . .
33
6.7
Optimizing cutoﬀc (Algorithm 5)
. . . . . . . . . . . . . . . . .
34
6.8
Overall complexity . . . . . . . . . . . . . . . . . . . . . . . . . .
34
7
Uniting Two Classiﬁcation Algorithms
36
8
Conclusion
39
2

===== Page 4 =====

1
Acknowledgements
I would like to thank Zolt´an Sasv´ari for carefully reading the manuscript and
for providing the environment which made this research possible.
Research leading to these results has received funding from the iRel40 project.
iRel40 is a European co-funded innovation project that has been granted by
the ECSEL Joint Undertaking (JU) under grant agreement no 876659. The
funding of the project comes from the Horizon 2020 research programme and
participating countries. National funding is provided by Germany, including the
Free States of Saxony and Thuringia, Austria, Belgium, Finland, France, Italy,
the Netherlands, Slovakia, Spain, Sweden, and Turkey.
Disclaimer: The document reﬂects only the author’s view and the JU is not
responsible for any use that may be made of the information it contains.
2
Introduction
In this paper we propose a fast stochastic method for object classiﬁcation which
can be applied to detecting and predicting defective semiconductor devices by
analyzing high-dimensional measurement data from wafer fabrication. The most
important goal is predicting early in the production chain which semiconductor
chips of a set of wafers will turn out to be defective later.
Our approach is interesting in that it led to good to excellent results in
frontend classiﬁcation tasks with real-world wafer data and is based on distributional analysis of the input data with quasilinear time complexity displaying
computation times of some minutes for typical classiﬁcation tasks of ours on
normal PC hardware.
The function principles we are using in the classiﬁcation algorithm presented
in this paper are deriving certain distributional properties from the scaled features of two sets of positive and negative samples in order to select a subset of
features with speciﬁc properties as candidate indicator features. These latter
features are then used for attempting to classify objects hitherto unknown to
the algorithm.
We will describe example applications of how this algorithm is used for
3

===== Page 5 =====

detecting frontend defects using only tiny sample sets. Another application we
will present here is trying to predict backend defects using only a very small
number of frontend measurements.
Part of the tasks we are tackling here have also been attacked by the algorithm described in [7] so we can compare the quality of detection or prediction by
these two algorithms which are based on completely diﬀerent principles. Similar
as with the latter algorithm and unlike, for example, typical neural-net classiﬁcation methods [8] with back-propagation, we try to avoid optimizations over
high-dimensional search ranges and prefer ﬁnding some best rater in a limited
set of raters over trying to ﬁnd a good rater in a giant search range.
The algorithms presented in this paper have quasi-linear time complexity.
They have been implemented in Python and have been used successfully for
classifying close to hundred thousand semiconductor chips of diﬀerent product
types, where classifying 10000 chips took some minutes on normal PC hardware.
We expect that re-implementation in C/C++ and parallelization would bring a
considerable speedup.
The main application of the classiﬁcation algorithm described in this paper
is improving quality control in wafer fabrication. In an ongoing series of research
projects working towards this goal, the following properties—already mentioned
in [7]—have turned out to be important for some algorithm to be economically
useful:
1. Not dependent on Gaussian distribution of some or all features.
2. High TP/FP quotients: as few as possible good chips should be scrapped
for sorting out defective chips.
3. Eﬃciency: necessity of classiﬁcation in fabrication real time.
4. Ability of coping with large amount of data, for example ≈30 −100 MiB
per data lot.
5. Means for reducing the number of tests: measurements are costly and time
consuming to diﬀerent extents.
Algorithm 6 and its auxiliary functions explained in section 3 have been designed
to meet these goals.
In some sense, this paper can be seen as a follow-up to [7]. Whereas in the
latter paper we reported on frontend detection and prediction tasks only, in this
paper we also consider backend predictions based on frontend measurements,
using a classiﬁcation method based on completely diﬀerent principles.
2.1
Overview
The rest of this paper is organized as follows. In section 3 we will deﬁne notations
and describe our algorithms. In section 4 we describe the experimental setup,
the tasks to be solved and the data material. In section 5 we will list and analyse
a number of results we obtained from a larger number of runs attempting to
4

===== Page 6 =====

classify semiconductor chips with respect to the overall defect state of one of
several possible measurement steps as the main application, and results from
classifying iris data. We will also propose and apply a feature relevance indicator
in section 5. Section 6 will be about the worst-case complexity of the algorithms
described in section 3. In section 7 we will propose an algorithm which unites
the classiﬁcation Algorithm 6 of this paper with the classiﬁcation algorithm
described in [7] by implanting some main ingredients of the latter algorithm
into the former. Section 8 will close this paper by concluding thoughts.
5

===== Page 7 =====

3
The Algorithms
3.1
Deﬁnitions
In what follows, we will use the abbreviations B = {0, 1}, Q∗= Q ∪{−∞, ∞}.
Let X ⊆Qm×n be a data matrix describing m objects by n numbers each.
3.1.1
Scaling
To begin with, the ﬁrst step of our classiﬁcation algorithm is column-wise scaling
the input data matrix X. Scale(X) refers to the following function.
Input/Output: X ∈Qm×n
1 for j = 1 . . . n do
2
compute µj, σj
// mean and standard deviation of the j-th column of X
3 end
4 for i = 1 . . . m do
5
for j = 1 . . . n do
6
xi,j =
( xi,j−µj
σj
,
σj ̸= 0
0,
σj = 0
7
end
8 end
Algorithm 1: Scale(X)
Every column of X has mean 0 and standard deviation 1 after scaling. After
scaling X, the algorithms do not operate on the original rows xi,1, . . . , xi,n) but
on the corresponding numbers that quantify how far (by how many standard
deviations σj) xi,j is away from the mean µj of its column j.
Note. We always assume Scale(X) ∈Qm×n. This is the case if Scale(X) is
computed numerically using ﬂoating point arithmetics. With symbolic computation, the element type of Scale(X) would be an extension ﬁeld Q(σ1, . . . , σn).
3.1.2
Histogram Functions
We consider the input objects—represented by lines of the input data matrix
X—as samples of random processes the true distributions of which are unknown
to us. Furthermore, we consider positive objects as results of a random process
generating positive objects and negative objects as results of a possibly diﬀerent random process generating negative objects. For guessing certain statistical
properties of the true distributions, we need a means to re-construct approximations of the shape of the true distributions. For doing this, we use histograms
derived from the sample objects T +, T −which will be described now.
Let v ∈Bm be a 0-1 column vector. We call the object belonging to line i a
6

===== Page 8 =====

positive object if vi = 1 and a negative object if vi = 0. Let
I+ = {i ∈1, . . . , m | vi = 1}
and
I−= {i ∈1, . . . , m | vi = 0}
be the sets of line indices belonging to all positive resp. negative objects. Let
T + ⊆I+ and T −⊆I−be two subsets which will serve as index sets of a set of
positive training objects and a set of negative training objects. Let be
I¬T = {1, . . . , m} \ (T + ∪T −)
the index set of objects outside of the training sets. This is the set of indices
of all objects to be classiﬁed. We will skip training objects in the classiﬁcation
loop for two reasons: ﬁrstly, in order to keep a clean separation of what is
known to the algorithm (v(i) for i ∈I¬T ) and what is to be predicted (v(i) for
i ̸∈T + ∪T −). And secondly, we consider classifying objects already known to
the algorithm as a diﬀerent, simpler problem than classifying objects not yet
known to the algorithm. Later, we will also need the index set of all training
objects
IT = T + ∪T −.
Since what we actually will be doing is using these two sets of training objects
as samples for guessing certain statistical properties of the full distributions
of the input data, we can call the two training sets positive samples and
negative samples as well.
For convenience and if there is no danger of misinterpretation, we will not
always make a diﬀerence when referring to objects and referring to lines of X.
We assume nb ≧3 (“number of bins”) to be a constant which will be used
for setting the number of bins in computing all occurring histograms.
Let a be an increasing sequence of numbers
a = (−∞< a1 < a2 < · · · < anb−1 < ∞) ∈{−∞} × Q∗nb−1 × {∞}
whereby we always set a0 = −∞, anb = ∞.
Every such a induces a partitioning
Q∗= ⊔nb−1
k=0 Ii
into mostly half-open intervals1 where
I0
=
(a0, a1)
=
(−∞, a1)
I1
=
[a1, a2)
. . .
. . .
Ii
=
[ai, ai+1)
(i = 0, . . . , nb −1)
. . .
Inb−2
=
[anb−2, anb−1)
Inb−1
=
[anb−1, anb)
=
[anb−1, ∞)
1The only exception being the ﬁrst interval I0 = (−∞, a1) which is open at both ends.
7

===== Page 9 =====

The assumption nb ≧3 guarantees that there is at least one interval [a1, a2)
with both limits being ﬁnite.
Deﬁnition. Let Ha,nb : Qm →Qnb be the histogram function that, given a
sequence of numbers x = (x1, . . . , xm) and a sequence of boundaries a = (−∞<
a1 < a2 < · · · < anb−1 < ∞), assigns to every bin index k ∈{0, 1, . . . , nb −1}
the relative frequency of this bin, i.e. the number of elements lying in Ik divided
by m:
Ha,nb(x) = (h0, . . . , hnb−1)
where
hk = 1
m ·
{i ∈{1, . . . , m} | xi ∈Ik}

(k = 0, 1, . . . , nb −1)
Obviously, Pnb−1
k=0 hk = 1. Note that a is required to be in strictly ascending
order whereas x may be unordered.
Given nb and x ∈Qm, we can specify boundaries a∗= (−∞< a∗
1 < a∗
2 <
· · · < a∗
nb−1 < ∞) in a way that the inner intervalls I∗
1, . . . , I∗
nb−2 have equal
width max(x)−min(x)
nb−2
. We just set a∗
1 = min(x) and a∗
nb−1 = max(x) + ε for some
ε > 0. Then
[min(x), max(x) + ε) = ⊔nb−2
k=1 I∗
i
When using these boundaries a∗, the outmost intervals I0 = (−∞, a∗
1) and
Inb−1 = [a∗
nb−1, ∞) do not contain any element of x and thus h0 = hnb−1 = 0
in this case. Every nb setting induces a function which maps x ∈Qm to the
special boundaries vector a∗deﬁned above:
a∗
nb : Qm
−→
Q∗nb+1
a∗
nb(x)
=
(−∞, a∗
1, a∗
2, . . . , a∗
nb−1, ∞)
Proposition. Let j ∈{1, . . . , n} be some column index. Let x1,j, . . . , xm,j
be independent and identically distributed with distribution P (j).
Let I =
{i1, . . . , is} ⊆{1, . . . , m} some ﬁxed subset of indices and HI
a,nb : Qs −→Qnb,
HI
a,nb(xi1,j, . . . , xis,j) = (hI
0, . . . , hI
nb−1) the histogram of (xi1,j, . . . , xis,j). Then
hI
k is an unbiased estimator of P

xi,j ∈[ak, ak+1)

for k = 0, . . . , nb −1.2
Proof. Let k ∈{0, . . . , nb −1} be ﬁxed. Deﬁne Xi1, . . . , Xis by
Xig =
(
1,
xig ∈[ak, ak+1)
0,
else
Then by linearity of the expectation E:
E

hI
k

= E
Xi1 + · · · + Xis
s

= E [Xi1] = P

xi,j ∈[ak, ak+1)

,
qed.
Thus by using T + ∪T −for I, we can take histograms derived from the training
set as unbiased estimators for the distribution of every column j.
2As an exception, the left-most interval (a0, a1) = (−∞, a1) is open on the left.
8

===== Page 10 =====

3.1.3
Finding Candidate Indicator Columns
Input: Scale(X) ∈Qm×n, T + ⊆I+, T −⊆I−, b+ ∈Q, b−∈Q, nb ∈
N≧3
Output: list of Cics L = [(js, a∗
ks, a∗
ks+1) | s = 1, . . . , S]
1 for j = 1 . . . n do
2
set x+ = (xi,j | i ∈T +)
3
compute a∗= a∗
nb(x+)
4
compute h+ = Ha∗,nb(x+) ∈Qnb
5
h+
max = max(h+
0 , . . . , h+
nb−1)
6
k+ = min{k ∈{0, . . . , nb −1} | h+
k = h+
max}
7
set x−= (xi,j | i ∈T −)
8
compute h−= Ha∗,nb(x−) ∈Qnb
9
if h+
max > b+ ∧h−
k+ < b−then
10
append (j, a∗
k+, a∗
k++1) to L
11
end
12 end
Algorithm 2: FindCics(X, T +, T −, b+, b−, nb)
FindCics(X, T +, T −, b+, b−, nb) takes as input the scaled input data matrix
Scale(X), two indices sets of positive and negative training objects, respectively, a lower bound b+ and an upper bound b−, and the number of bins nb to
be used in the histograms.
Deﬁnition. A candidate indicator column (Cic) is any column j of
Scale(X) which has the property that the most-frequent bin Ik+ of the column
j entries of all lines belonging to positive samples is > b+ whereas the same bin
Ik+ (i.e. with the same boundaries) of the column j entries of all lines belonging
to negative samples is < b−.
If there are multiple k ∈{0, . . . , nb −1} with h+
k = h+
max then we take the
one k with the smallest index in the above algorithm 2:
k+ = min{k | h+
k = h+
max}.
In Ik+ = [a∗
k+, a∗
k++1), the 2nd and 3rd entry of the triplet (j, a∗
k+, a∗
k++1) being
appended to L are the lower and upper boundary of the most-frequent bin Ik+
of the histogram of positive objects h+.
Clearly, FindCics(Scale(X), T +, T −, b+, b−, nb) returns the list of all Cics of
Scale(X).
3.1.4
Computing Indicator Values SC(i)
Deﬁnition. Let C = FindCics(Scale(X), T +, T −, b+, b−, nb) be the output of
Algorithm 2. Then deﬁne
SC : I¬T
−→
N≧0
SC(i)
=
|{(j, a∗
k+, a∗
k++1) ∈C | xi,j ∈[a∗
k+, a∗
k++1)}|
9

===== Page 11 =====

In words, SC assigns to every index i of a non-training object (xi,1, . . . , xi,n) the
number of those Cics j for which the j-th column entry of this object lies in the
most-frequent bin of the histogram of positive samples.
Input: C = FindCics(Scale(X), T +, T −, b+, b−, nb)
Output: SC(I¬T )
1 for i ∈I¬T do
2
compute SC(i) = |{(j, a∗
k+, a∗
k++1) ∈C | xi,j ∈[a∗
k+, a∗
k++1)}|
3 end
Algorithm 3: Computing indicator values SC(i)
3.1.5
Computing Predictions F(i, c)
After having computed an indicator value SC(i) for every object i outside the
training sets we need to obtain binary predictions, 1 for predicting object i to
be positive, 0 for negative:
Input: SC
 I¬T

as computed by Algorithm 3, c ≧0
Output: F(c) ∈B|I¬T |
1 for i ∈{1, . . . , m} \ (T + ∪T −) (= I¬T ) do
2
F(i, c) =
(
1,
SC(i) ≧c
0,
SC(i) < c
3 end
Algorithm 4: Computing predictions
Of the wide range of methods for specifying C we desribe just two.
3.1.6
CutoﬀSelection
Let be
I+
¬T
=
I+ ∩I¬T
I−
¬T
=
I−∩I¬T
the index sets of objects to be classiﬁed which are positive or negative, respectively, in truth. Their average indicator values are:
Av1
=
1
|I+
¬T | ·
X
i∈I+
¬T
SC(i)
Av0
=
1
|I−
¬T | ·
X
i∈I−
¬T
SC(i)
10

===== Page 12 =====

In what follows, we assume3 Av1 −Av0 ≧0.
Na¨ıve cutoﬀselection.
c = Av1 −Av0
2
This better-than-nothing selection can be computed very fast but may make
predictions F(i, c) far from what can be achieved with a more sophisticated
cutoﬀ.
Optimizing cutoﬀfor some statistical quantity Q(v, w).
Let
 i1, . . . , i|I¬T |
be an enumeration of I¬T in some ﬁxed order and
v =
 v(i1), . . . , v(i|I¬T |)

the 0-1 vector of their true positive/negative states. Let be
F(c) =
 F(i1, c), . . . , F(i|I¬T |, c)

the 0-1 vector of all F(i, c) for i ∈I¬T using the same index ordering as in
v. Furthermore, let Q(v, w) be some statistical quantity measuring similarity
of two 0-1 vectors v and w. This may be accuracy4 or—by considering F(c)
and v as two raters of the same feature vector—Cohen’s kappa5, among other
possibilities.
Denote
Smin
C
=
min SC (I¬T )
Smax
C
=
max SC (I¬T )
3If this does not hold true in a speciﬁc application then this may be an indication of
insuﬃcient training sets: not characteristic enough, or with contradictive positive/negative
bits v(i).
4Accuracy is the amount of coincident bits of two r-bit vectors:
accu(v, w) = 1
r ·
r
X
i=1
 vi · wi + (1 −vi)(1 −wi)

5Cohen’s Kappa [2] is used for measuring agreement of two binary r-bit raters v and w
and deﬁned as
kappa(v, w) = accu(v, w) −pe
1 −pe
where
pe = 1
r2 (nvi=0 · nwi=0 + nvi=1 · nwi=1).
11

===== Page 13 =====

Input: C = FindCics(Scale(X), T +, T −, b+, b−, nb)
Output: Copt ∈Q, F(Copt) ∈B|I¬T |
1 for c = Smin
C
, Smin
C
+ 1, . . . , Smax
C
do
2
compute F(c) by Algorithm 4
3
compute Q
 F(c), v

4 end
5 Qopt = max{Q
 F(c), v

| c ∈C}
6 Copt = argmax(Qopt)
Algorithm 5: Optimizing cutoﬀc
Since the indicator values SC(i) are small integers which are limited to the
number of Cics there is little practical advantage in employing more reﬁned
methods than Algorithm 5 for ﬁnding an optimal cutoﬀc like the hill-climbing
approach described in the subsection ”Reﬁned Method For CutoﬀSelection”
of [7].
If the optimal cutoﬀcan not be computed for the whole set of objects because
only some subset of all objects is known to the user then the cutoﬀoptimization
has to be performed for the known set of objects ﬁrst and used as an estimator
for the whole lot of data later.
3.1.7
Classiﬁcation Algorithm
By putting it all together, we arrive at the following algorithm for object classiﬁcation.
Input: X ∈Qm×n; b+, b−∈Q; nb ∈N≧3; t+, t−∈N≧1
Output: F(Copt) ∈B|I¬T |
1 Scale(X) by Algorithm 1
2 select random subsets T + ⊆I+ of size t+, T −⊆I−of size t−
3 C := FindCics(Scale(X), T +, T −, b+, b−, nb) by Algorithm 2
4 compute SC(I¬T ) by Algorithm 3
5 Smin
C
:= min SC(I¬T )
6 Smax
C
:= max SC(I¬T )
7 compute Copt and F(Copt) by Algorithm 5
Algorithm 6: Classiﬁcation algorithm
3.2
Free Parameters: b+, b−And nb
b+ and b−specify which features are to be considered as Cics in the deﬁnition
of Cic on page 9. They are used in Algorithm 2. In practice, the value of b+
seems to be far more important than b−as most small positive values b−≈
> 0
12

===== Page 14 =====

served equally well in our applications.
In section 5.7, we will present Algorithm 7 as an alternative for Algorithm 2
in order to not control the selection of Cics by b+ and b−but by specifying how
many of the top ranks according to their ndiﬀvalue are to be used as indicator
columns. This replaces b+, b−—which are obsolete then—by a new parameter t
which in turn may be obsoleted by deﬁning a convenient default value depending
on the feature count of the input data matrix X.
nb speciﬁes how many intervals are to be ﬁlled by the histogram function
Ha,nb : Qm −→Qnb deﬁned on page 8.
Note that the output of Scale(X)—which serves as input to Ha,nb— may
be quantized already, ﬁrstly because every continuous feature of the input data
matrix X ∈Qm,n—measurements of an analog quantity, for example—is already quantized, and secondly by the well-known limitations of ﬂoating point
processing and the number format of X and Scale(X). As is typical of algorithms which are processing real-world data using ﬂoating point, quantizations
may be obvious (number format of numerical data in ﬁles, for example) or silent
(pre-quantized measurement data, limitations of ﬂoating point processing). In
our chip classiﬁcations, we limited the entries of Scale(X) to 3 signiﬁcant digits
with exponent notation if necessary and set nb = 1000 or 2000.
3.3
Considerations on µj And σj
In practical applications, the means µj and the standard deviations σj (j =
1, . . . , n) of the total population as they are needed in the scaling Algorithm 1
may not be known in advance. We can obtain canonical estimations ¯µj and ¯σj
by computing the sample mean and sample variance of the training objects, i.e.
¯µj
=
1
n ·
X
i∈IT
xi,j
s2
j
=
1
n −1 ·
X
i∈IT
(xi,j −µj)2
where IT = T + ∪T −. It is well-known that ¯µj is an unbiased estimator for
µj and s2
j is an unbiased estimator for the variance σ2
j . For concave functions
f of some random variable X it holds E[f(X)] ≦f(E[X]) but not necessarily
equality. Since the square root is a concave function, sj :=
q
s2
j may not be
an unbiased estimator for σj and generally, there is no formula for unbiased σj
estimation being true for all distributions. In case the bias of the estimation
of σj being especially low is of elevated importance there are improved estimators. See, for example, [1] and the references therein on using the estimator
r
1
n−1.5 · P
i∈IT
(xi,j −µj)2 in case of normal distribution.
Of course, there may be good reasons for decoupling the sample set for
estimating µj and σj from T + and T −by choosing a diﬀerent set than IT for
13

===== Page 15 =====

the former because the size of IT may be inﬂuenced by considerations of avoiding
over- and undersampling in the classiﬁcation.
Note. It proved useful in our applications to have the possibility to optionally bypass the FindCics stage (Algorithm 2) by implementing a means to
speciﬁy the Cics manually, especially in case of relevant Cics being known in
advance from former runs with similar data. Then the C in SC is not the output
of FindCics(. . .) but a list of triples (js, a∗
ks, a∗
ks+1) created for those columns js
speciﬁed by the user. This has been applied successfully in cases where several
lots of measurement data are to be classiﬁed whereby the chips are of the same
product type.
3.4
Limits
Clearly, the classiﬁcation Algorithm 6 requires the existence of some columns
of X where the conditional distribution on the positive samples diﬀers from
the conditional distribution on the negative samples. In a stochastic sense, this
presupposes the existence of some features the true distributions of which diﬀer
when restricted to the positive and to the negative objects separately.
14

===== Page 16 =====

4
Experimental Setup
4.1
Task And Data
The tasks to be solved are as described in [7]:
Given measurements and the defect states for chips of a small training
set, predict the defect state of the remaining chips of the lot based on
the measurements only.
The data we are given consist of measurement data from chip fabrication. The
data is organized in lots where one lot is a set of wafers from production. Each
wafer carries a ﬁxed number of chips of the same product type.
In what follows, we ignore the partitioning of a lot into wafers and consider a
lot as a series of m chips of the same product type where each chip is represented
by n measurements.
Using the notation of subsection 3.1, the input to our
algorithm consist of:
−X ∈Qm×n where the i-th line represents the i-th chip and the j-th column
contains the j-th measurement
−v ∈Bn where vi is the true defect state of the i-th chip.
Every column represents a feature because measurements in the same column
are presupposed to be of the same type.
The general hypothesis the classiﬁcation algorithm presented in [7] is based
on is that defective devices may possess abnormalities in patterns derived from
deviations by thresholding from the component-wise means of measurement
data. The approach presented in this paper in turn resides on the hypothesis
that there may exist features—columns of X in the above algorithms—which we
call Cics where defective devices diﬀer from normal devices in their distributional
properties.
The columns of X are grouped into so-called MeasSteps (measurement
steps) named S1, S2, ... for front-end MeasSteps. Every column of X belongs
to one unique MeasStep. To every chip and every MeasStep there is assigned
either an error code specifying the type of defect which had occurred in this step,
or ”0” meaning “passed”. In addition to this, if the tasks consists in predicting
backend defects then to every chip there is assigned a backend error code or
”BEpass”.
In the tasks we are describing here, the true defect state vi ∈Bm of the i-th
chip may be
−the frontend defect state “anything but ”0”” or
−the backend defect state “anything but ”BEpass”” in our tasks.
Notation. We will occasionally abbreviate “frontend” as FE and “backend”
as BE.
15

===== Page 17 =====

The research problem to be solved is: Find algorithms that are able to learn
the function f : Qn −→B which maps the vector of measurements of the i-th
chip to the defect state of this chip:
(xi,1, . . . , xi,n) 7→vi
So, given (xi,1, . . . , xi,n) for a small set of training chips with indices i ∈T +∪T −
we want an algorithm that predicts v¯i for chips of the same product type with defect state yet unknown, knowing only their measurement vectors (x¯i,1, . . . , x¯i,n).
We are not only interested in predicting defect states of chips when a full set
of measurements is available but also in predictions when only part of measurement data is available. This bears the possibilites of saving costs by reducing
the amount of measurements and predicting defects earlier in the production
process.
4.2
The Data Basis
In our experiments we considered chips of 4 diﬀerent product types A, B, C and
D with the following properties:
Product
#Chips
#Measurements per chip
continuous only
A
8280
385/1084
yes/no
B
11952
332
yes
C
11328
915
no
D
34550
150
no
The data of Product B contains only continuous features (currents, voltages,
...) whereas products C and D also include discrete features (ﬂag words, counts,
...).
An important feature of our method for object classiﬁcation is that we do not
use any meta-knowledge about the data. For chip measurement data, this means
that we do not know the types or units nor the meanings of the measurements.
All we know is their numerical values.
4.3
The Implementation
A program for the application described in subsection 4.1 including several extra
features (graphical output etc.) has been realized in 3400 lines of Python 3 using
the NumPy [5] and Matplotlib [6] packages.
The algorithms presented above
contain several loops which could be parallelized by using the multiprocessing
package: the ﬁrst j loop and the i loop in Algorithm 1, the j loop in Algorithm
2, the i loop in Algorithm 3 and the c loop in 5. We keep this optimization for
future versions of this implementation.
16

===== Page 18 =====

5
Results
5.1
Properties
The main Algorithm 6 meets all of the ﬁve goals listed in section 2 which seems
to make it well-suited for detecting or predicting the overall frontend defect
states of chips using measurement data.
When using the classiﬁcation algorithm presented here together with the dimensional reduction method dim-reduce
 Scale(X), sharpness

described in subsection “Dimensional Reduction” of [7] as a preprocessing step, the algorithm
presented here computed better results than the algorithm of [7] with the dimensional reduction in a series of classiﬁcation runs on the same input. In what
follows, we refer to this dimensional reduction by dim-reduce. See [7] for more
details. See subsection 5.3.3 for numbers.
The hardware demands of the algorithms presented in this paper are low.
All classiﬁcations the results of which are listed in this paper together took 22
minutes on a Pentium i5-750 using a Python implementation of Algorithm 6
without parallelization on task- or thread-level.
5.2
Notational Remark
We will abbreviate “true positive” as TP, “false positive” as FP, “true negative”
as TN and “false negative” as FN henceforth.
5.3
Predicting or Detecting Frontend Defect Status of
Chips
5.3.1
Predicting Frontend Defect Status S2 of Chips
The goal of this application is to predict the frontend overall defect state ”0”
of MeasStep S2 if the S1 and S2 measurements are known to the classiﬁcation
algorithm. In the following example, we are going to classify 11328 chips of
Product C using 20%/5% of all positive/negative chips for training, setting
b+ = 0.3. The prediction quality is summarized by the 4 ﬁelds and their various
quotients in the following table:
17

===== Page 19 =====

TP
477
FP
2
TN
10175
FN
15
TP/(TP+FN)%
97.0
TN/(TN+FP)%
100.0
FP/(TP+FN)%
0.4
FN/(TN+FP)%
0.1
TP/FP
238.5
TN/FN
678.3
Accuracy%
99.8
Kappa
0.982
The next two ﬁgures are created by the Algorithms 3 and 5 while solving
the aforementioned task. Figure 1 shows the value SC(i) for each of the 10669
objects i to be classiﬁed.
Note. For the beholder’s convience, all positive objects i are rearranged to
the left in this diagram whereas all negative objects i are moved to the right.
So the diﬀerences in SC(i) of positive objects and negative objects are clearly
visible. This will be done in all diagrams of this type from now on. Figure 2
Figure 1: SC(i) in the S2 classiﬁcation with product C.
0
2000
4000
6000
8000
10000
Object
0
50
100
150
200
250
300
NumSat [sampled]
Number of conditions satisfied per object
knowing only sampled distribution.
shows the plots of cutoﬀ-vs-accuracy and cutoﬀ-vs-kappa which are created by
Algorithm 5 while ﬁnding some cutoﬀCopt for optimizing accuracy or kappa.
18

===== Page 20 =====

The kappa value of this result is slightly better than the value (0.979) computed
Figure 2: Cutoﬀoptimization of the S2 classiﬁcation with product C.
0
50
100
150
200
250
300
Cutoff
0.0
0.2
0.4
0.6
0.8
1.0
Accuracy
0
50
100
150
200
250
300
Cutoff
0.0
0.2
0.4
0.6
0.8
1.0
Kappa
Cutoff vs Accuracy (left), cutoff vs Kappa (right) knowing only samples.
by the algorithm of [7]. All in all, this task is easy to both algorithms even
though they diﬀer vastly in the function principles they are based on.
As another illustration of the performance of Algorithm 6 at predicting frontend defects, we will show next the results of classifying 34550 chips of product
D predicting the overall defect state ”0” of the second MeasStep S2. We used
50%/10% of all positive/negative objects as samples for training.
TP
740
FP
0
TN
31347
FN
36
TP/(TP+FN)%
95.4
TN/(TN+FP)%
100.0
FP/(TP+FN)%
0.0
FN/(TN+FP)%
0.1
TP/FP
∞
TN/FN
870.8
Accuracy%
99.9
Kappa
0.976
The latter result has been achieved by Algorithm 6 using only 3 columns
as Cics. We used the same 3 columns here as we did in our experiment for
minimizing the number of input features for BE defect prediction, set out in
subsection 5.4.2. When using only 2 columns for Cics or specifying the Cics by
b+, the resulting quality of predictions was only marginally diﬀerent.
The limits mentioned in subsection 3.4 of the distributional analysis approach of Algorithm 6 showed up when classiﬁying Product A. Unlike with the
19

===== Page 21 =====

algorithm of [7] where 5000 chips could be classiﬁed with a kappa value of 0.707
and accuracy 98.3%, using Algorithm 6 the best result with the same positive/negative training set sizes 70%/10% was kappa 0.469 and accuracy 96.3%.
In accordance with this, looking at the histograms of Product A, one sees that
when leaving out all discrete features, the positive and negative histograms of
many features actually do not seem to diﬀer much in a way that histogram analysis could detect with some certainty. Classifying Product A was also somewhat
harder than classifying the other products mentioned there to the classiﬁcation
algorithm of [7].
5.3.2
Detecting Frontend Defect Status S1 of Chips
In this application, the task was detecting the frontend defect status S1 of 10000
chips of product B while knowing measurements of S1 and S2. In order to know
how the detection quality depends on the training set sizes, we tested a wide
range of sizes. In the following table we list a small representative selection of
results. We set nb to 2000 and b+ to 0.98 throughout this series.
Train
Train
FP
FN
Kappa
Pos%
Neg%
90
10
0
3
0.979
50
10
1
10
0.983
20
10
1
14
0.985
1
10
16
21
0.970
0.125
10
16
21
0.971
0.125
1
5
22
0.979
0.125
0.125
0
23
0.982
0.125
0.01
10
12
0.983
0.125
0.005
1
23
0.981
0.1
10
278
91
0.741
This table shows that the detection quality is hardly sensitive to the selection of
the training set sizes in this series. Remarkably, when setting TrainPos=0.125%
and TrainNeg=0.005%, the training sets consist of only 3 chips: 2 positive
samples and 1 negative sample.
Figure 3 shows the plots of cutoﬀ-vs-accuracy and cutoﬀ-vs-kappa which
are created by Algorithm 5 while classifying 10000 chips of product B using 2
positive sample chips and 1 negative sample chip for training.
The next 3 ﬁgures show some histograms of the above series, created by
Algorithm 2. In each of the 3 ﬁgures, there are 25 diagrams in a 5×5 grid. The
ﬁve rows correspond to columns #81 to #85 of the matrix Scale(X). There are
5 histograms in each row, each rendered by using 100 bins:
1. the (signed) diﬀerence of the pos and neg histograms
2. their absolute diﬀerence
3. the histogram of positive objects (“pos”)
20

===== Page 22 =====

Figure 3: Cutoﬀoptimization of the S1 classiﬁcation with product B.
0
10
20
30
40
Cutoff
0.2
0.4
0.6
0.8
1.0
Accuracy
0
10
20
30
40
Cutoff
0.0
0.2
0.4
0.6
0.8
1.0
Kappa
Cutoff vs Accuracy (left), cutoff vs Kappa (right) knowing only samples.
4. the histogram of negative objects (“neg”)
5. the histogram of all objects (“all”)
Figure 4 shows how the histograms would be looking like if the classiﬁcation
algorithm would get to see all objects as input data. In real applications the
algorithms gets to see only the training sets. So in the next Figure 5 the histograms are displayed when using 20%/10% of all objects as samples for the
positive/negative training sets. The topmost row displays column #81 which is
not used as a Cic (see page 9) in classiﬁcation, whereas the next 4 rows (displaying columns #82 to #85) are used as Cics. In accordance with this, if we
compare the 3rd column of Figure 5 with the 4th column, we see that in the 4th
column, the peak of the topmost diagram is much higher than in the remaining
4 diagrams of this column. This means, the diﬀerence in relative frequencies
of the most frequent bin of the “pos” histogram (column 3) and the “neg” histogram (column 4) is much lower in column #81 than in the columns #82 to
#85. Thus Algorithm 2 will use columns #82 to #85 as Cics, but not #81.
Finally, in Figure 6 we see the histograms created just like the former, but
this time using only 4 chips for training: 2 positive objects and 2 negative objects. As one can see, the only relative frequencies occurring are 0, 0.25, 0.5, 0.75
and 1. Again, by comparing the diagrams of the 3rd column (“pos”) with the
diagrams of the 4th column (“neg”), we see that the peaks in the 4 bottom
“pos” histograms are located diﬀerently than in the belonging 4 bottom “neg”
histograms of the same row. But this is not the case when comparing the topmost “pos” with the topmost “neg” histogram, which again gives a hint why
the FindCics Algorithm 2 takes columns #82 to #85 as Cics, but not #81, even
when knowing only 2+2 samples for training.
21

===== Page 23 =====

Figure 4: Histogram collection of 5 features, knowing all objects.
0.05
0.05
0.05
−0.5
0.0
0.5
neg-pos #81 |14.52|
'0.00' :-2.28:
0.05
0.05
0.05
0.0
0.5
|neg-pos| #81
-2.68
1.59
5.86
0.0
0.1
all #81 (0.00) [0.05] {1.00}
-2.68
1.59
5.86
0
1
pos #81 (0.05) [0.05]
{0.00} :-2.00:
-2.68
1.59
5.86
0.0
0.1
neg #81 (-0.00) [0.05]
{1.04} :0.28:
-0.13
-0.13
-0.13
−0.5
0.0
0.5
neg-pos #82 |14.52|
'-0.00' :-2.34:
-0.13
-0.13
-0.13
0.0
0.5
|neg-pos| #82
-2.62
0.90
4.41
0.00
0.05
all #82 (-0.00) [-0.13] {1.00}
-2.62
0.90
4.41
0
1
pos #82 (-0.13) [-0.13]
{0.00} :-2.00:
-2.62
0.90
4.41
0.00
0.02
neg #82 (0.01) [-0.13]
{1.04} :0.34:
-0.14
-0.14
-0.14
−0.5
0.0
0.5
neg-pos #83 |14.52|
'-0.00' :-2.52:
-0.14
-0.14
-0.14
0.0
0.5
|neg-pos| #83
-3.03
0.71
4.45
0.00
0.05
0.10
all #83 (-0.00) [-0.14] {1.00}
-3.03
0.71
4.45
0
1
pos #83 (-0.14) [-0.14]
{0.00} :-2.00:
-3.03
0.71
4.45
0.000
0.025
neg #83 (0.01) [-0.14]
{1.04} :0.52:
-0.12
-0.12
-0.12
−0.5
0.0
0.5
neg-pos #84 |14.52|
'-0.00' :-2.41:
-0.12
-0.12
-0.12
0.0
0.5
|neg-pos| #84
-2.79
0.81
4.40
0.0
0.1
all #84 (0.00) [-0.12] {1.00}
-2.79
0.81
4.40
0
1
pos #84 (-0.12) [-0.12]
{0.00} :-2.00:
-2.79
0.81
4.40
0.00
0.02
neg #84 (0.01) [-0.12]
{1.04} :0.41:
-0.13
-0.13
-0.13
−0.5
0.0
0.5
neg-pos #85 |14.52|
'-0.00' :-2.44:
-0.13
-0.13
-0.13
0.0
0.5
|neg-pos| #85
-2.88
0.77
4.41
0.0
0.1
all #85 (-0.00) [-0.13] {1.00}
-2.88
0.77
4.41
0
1
pos #85 (-0.13) [-0.13]
{0.00} :-2.00:
-2.88
0.77
4.41
0.000
0.025
neg #85 (0.01) [-0.13]
{1.04} :0.44:
DiffHistograms of columns 81-85  [whole distribution] [numBins=100]. (m) is mean, [c] is median, {s} is stddev.
|r| is relative deviation of mu_pos from mu_neg and 'r' is relative deviation of median_pos from median_neg.
:k: is kurtosis in the 'pos' and 'neg' plot, and the difference of the two in the difference plot.
5.3.3
Predicting Frontend Defect States Knowing Only Part of Measurement Data
In order to evaluate the suitability of the algorithm described in this paper
to the prediction of the frontend overall defect state ”0” with a successively
reduced set of features as input, we repeated the application described in section “Dimensional Reduction” of [7] with the same input data but replaced the
algorithm of loc. cit. by the classiﬁcation algorithm described in this paper.
After scaling the input matrix X by Scale(X) in Algorithm 6, we proceeded
by processing the scaled matrix by running dim-reduce
 Scale(X), sharpness

with increasing sharpness settings 0,1,2,3,4.
Both algorithms have been extended by this dim-reduce preprocessing.
The next two tables show the results of classifying 11328 chips of product
C (429 S1-measurements per chip). The algorithms are given measurements
of MeasStep S1 only, and the task is predicting the overall defect state ”0” of
MeasStep S2 without knowing any S2 measurements.
The following table shows the quality of prediction by the classiﬁcation algorithm of [7] using abs-t-excess thresholding.
22

===== Page 24 =====

Figure 5: Histogram collection of 5 features, knowing 20%/10% sample sets
only.
0.05
0.05
0.05
−0.5
0.0
0.5
neg-pos #81 |15.20|
'0.00' :-2.69:
0.05
0.05
0.05
0.0
0.5
|neg-pos| #81
-2.34
1.76
5.86
0.0
0.2
all #81 (0.00) [0.05] {0.99}
-2.34
1.76
5.86
0
1
pos #81 (0.05) [0.05]
{0.00} :-2.00:
-2.34
1.76
5.86
0.0
0.1
neg #81 (-0.00) [0.05]
{1.06} :0.69:
-0.13
-0.13
-0.13
−0.5
0.0
0.5
neg-pos #82 |8.04|
'0.09' :-2.64:
-0.13
-0.13
-0.13
0.0
0.5
|neg-pos| #82
-2.05
1.01
4.08
0.0
0.1
all #82 (-0.00) [-0.13] {1.00}
-2.05
1.01
4.08
0
1
pos #82 (-0.13) [-0.13]
{0.00} :-2.00:
-2.05
1.01
4.08
0.00
0.02
neg #82 (0.02) [-0.14]
{1.07} :0.64:
-0.14
-0.14
-0.14
−0.5
0.0
0.5
neg-pos #83 |8.77|
'-0.07' :-2.74:
-0.14
-0.14
-0.14
0.0
0.5
|neg-pos| #83
-2.27
1.03
4.34
0.0
0.1
all #83 (-0.00) [-0.14] {1.01}
-2.27
1.03
4.34
0
1
pos #83 (-0.14) [-0.14]
{0.00} :-2.00:
-2.27
1.03
4.34
0.000
0.025
neg #83 (0.02) [-0.13]
{1.09} :0.74:
-0.12
-0.12
-0.12
−0.5
0.0
0.5
neg-pos #84 |16.13|
'0.04' :-2.66:
-0.12
-0.12
-0.12
0.0
0.5
|neg-pos| #84
-2.31
1.05
4.40
0.0
0.1
all #84 (-0.01) [-0.12] {1.01}
-2.31
1.05
4.40
0
1
pos #84 (-0.12) [-0.12]
{0.00} :-2.00:
-2.31
1.05
4.40
0.000
0.025
neg #84 (0.01) [-0.13]
{1.08} :0.66:
-0.13
-0.13
-0.13
−0.5
0.0
0.5
neg-pos #85 |6.41|
'0.13' :-2.63:
-0.13
-0.13
-0.13
0.0
0.5
|neg-pos| #85
-2.41
1.00
4.41
0.0
0.1
all #85 (0.00) [-0.13] {1.02}
-2.41
1.00
4.41
0
1
pos #85 (-0.13) [-0.13]
{0.00} :-2.00:
-2.41
1.00
4.41
0.00
0.02
neg #85 (0.02) [-0.14]
{1.09} :0.63:
DiffHistograms of columns 81-85  [sampled distribution] [numBins=100]. (m) is mean, [c] is median, {s} is stddev.
|r| is relative deviation of mu_pos from mu_neg and 'r' is relative deviation of median_pos from median_neg.
:k: is kurtosis in the 'pos' and 'neg' plot, and the difference of the two in the difference plot.
Sharpness
#Features
%Features
Accuracy
Kappa
TP
of reduction
omitted
omitted
[zmin]
[zmin]
FP
0
0
0%
0.989
0.885
33.3
1
283
66.0%
0.974
0.664
+∞
2
346
80.7%
0.973
0.656
34.4
3
379
88.3%
0.946
0.194
0.234
4
387
90.2%
0.946
0.188
0.201
In this second table we list the corresponding results by the algorithm described
in this paper, using 50%/5% of all positive/negative chips as samples:
Sharpness
#Features
%Features
Accuracy
Kappa
TP
of reduction
omitted
omitted
FP
0
0
0%
0.997
0.936
272
1
283
66.0%
0.996
0.931
90.3
2
346
80.7%
0.988
0.769
9.5
3
379
88.3%
0.985
0.638
73.5
4
387
90.2%
0.856
0.135
0.1
As for the accuracy and kappa values, the algorithm described in this paper is
superior to the classiﬁcation algorithm of [7] in tackling this task with sharpness
0 to 3. For example, with sharpness 1—leaving out 66% of the features—the
23

===== Page 25 =====

Figure 6: Histogram collection of 5 features, knowing 2+2 sample objects only.
0.05
0.05
0.05
−0.5
0.0
0.5
neg-pos #81 |1.19|
'1.19' :0.00:
0.05
0.05
0.05
0.0
0.5
|neg-pos| #81
-0.63
-0.29
0.05
0.0
0.5
all #81 (-0.12) [0.05] {0.34}
-0.63
-0.29
0.05
0
1
pos #81 (0.05) [0.05]
{0.00} :-2.00:
-0.63
-0.29
0.05
0.0
0.5
neg #81 (-0.29) [-0.29]
{0.48} :-2.00:
-0.13
-0.13
-0.13
−0.5
0.0
0.5
neg-pos #82 |0.80|
'0.80' :0.00:
-0.13
-0.13
-0.13
0.0
0.5
|neg-pos| #82
-1.37
-0.65
0.08
0.0
0.5
all #82 (-0.39) [-0.13] {0.66}
-1.37
-0.65
0.08
0
1
pos #82 (-0.13) [-0.13]
{0.00} :-2.00:
-1.37
-0.65
0.08
0.0
0.5
neg #82 (-0.65) [-0.65]
{1.02} :-2.00:
-0.14
-0.14
-0.14
−0.5
0.0
0.5
neg-pos #83 |0.69|
'0.69' :0.00:
-0.14
-0.14
-0.14
0.0
0.5
|neg-pos| #83
-1.24
-0.44
0.36
0.0
0.5
all #83 (-0.29) [-0.14] {0.68}
-1.24
-0.44
0.36
0
1
pos #83 (-0.14) [-0.14]
{0.00} :-2.00:
-1.24
-0.44
0.36
0.0
0.5
neg #83 (-0.44) [-0.44]
{1.14} :-2.00:
-0.12
-0.12
-0.12
−0.5
0.0
0.5
neg-pos #84 |0.58|
'0.58' :0.00:
-0.12
-0.12
-0.12
0.0
0.5
|neg-pos| #84
-1.03
-0.29
0.45
0.0
0.5
all #84 (-0.20) [-0.12] {0.61}
-1.03
-0.29
0.45
0
1
pos #84 (-0.12) [-0.12]
{0.00} :-2.00:
-1.03
-0.29
0.45
0.0
0.5
neg #84 (-0.29) [-0.29]
{1.05} :-2.00:
-0.13
-0.13
-0.13
−0.5
0.0
0.5
neg-pos #85 |0.72|
'0.72' :0.00:
-0.13
-0.13
-0.13
0.0
0.5
|neg-pos| #85
-1.21
-0.45
0.32
0.0
0.5
all #85 (-0.29) [-0.13] {0.65}
-1.21
-0.45
0.32
0
1
pos #85 (-0.13) [-0.13]
{0.00} :-2.00:
-1.21
-0.45
0.32
0.0
0.5
neg #85 (-0.45) [-0.45]
{1.08} :-2.00:
DiffHistograms of columns 81-85  [sampled distribution] [numBins=100]. (m) is mean, [c] is median, {s} is stddev.
|r| is relative deviation of mu_pos from mu_neg and 'r' is relative deviation of median_pos from median_neg.
:k: is kurtosis in the 'pos' and 'neg' plot, and the difference of the two in the difference plot.
algorithm of this paper reaches a kappa value of 0.931 which is excellent in
comparison to 0.664 by the algorithm of [7].
With sharpness 3—leaving out 88.3% of the features—the cited algorithm
reaches a rather low kappa value of 0.194 and TP
FP < 1 whereas the algorithm of
this paper still reaches 0.638 and TP
FP > 73.
5.4
Predicting Backend Defect Status of Chips
All results described up to now were obtained by trying to detect or to predict the frontend (FE) overall defect state ”0” of chips from a series of wafers.
A related problem is trying to predict the backend (BE) overall defect state
”BEpass” knowing only frontend measurement data. The latter problem of BE
defect prediction has turned out to be harder than FE defect prediction or
detection to all algorithms we have tried.
Note. Whenever we write “predicting the BE overall defect state ”BEpass””,
what we actually are doing is predicting the negation of ”BEpass”. This is completely equivalent because the predictions are binary so negating the predicted
bit converts the former into the latter and vice versa.
24

===== Page 26 =====

5.4.1
Predicting Backend Defect Status of Chips Knowing Frontend
Data
In this application, input data consists of 150 measurements per chip taken at
diﬀerent frontend fabrication steps, and, as in the FE application, all chips are
of the same product type. The task consists in predicting the BE overall defect
state ”BEpass” for a large sets of chips after training with two smaller sets of
positive and negative samples, respectively. In addition, we wanted to know how
many and which measurements could be omitted without reducing the quality
of prediction unduly.
In the following two ﬁgures 7 and 8, we see plots created by Algorithm 6
while classifying 10000 chips of product D for BE overall defect status. See the
explanations for Figure 2 and 1 in subsection 5.3.1 for details on the type of
results these plots show. For this BE classiﬁcation of 10000 chips of product D
we set b+ = 0.41 and used 60% of all positive and 10% of all negative chips as
samples for training.
Figure 7: SC(i) in BE classiﬁcation with product D, using all features.
0
2000
4000
6000
8000
Object
0
1
2
3
4
5
6
7
NumSat [sampled]
Number of conditions satisfied per object
knowing only sampled distribution.
5.4.2
Predicting Backend Defect States Knowing Only Part of Measurement Data
Next, we tried how far we can reduce the number of features without inducing
inacceptable loss in quality of prediction. In result, we could shrink the input
data to just 2 speciﬁc features out of 150.
We used the same data lot and
25

===== Page 27 =====

Figure 8: Cutoﬀoptimization of BE classiﬁcation with product D, using all
features.
0
1
2
3
4
5
6
7
Cutoff
0.2
0.4
0.6
0.8
1.0
Accuracy
0
1
2
3
4
5
6
7
Cutoff
0.0
0.1
0.2
0.3
0.4
0.5
0.6
Kappa
Cutoff vs Accuracy (left), cutoff vs Kappa (right) knowing only samples.
the same amount of positive (60%) and negative (10%) chips for training as in
subsection 5.4.1 above. The plots in Figure 9 and 10 are created by Algorithm
6 using only these 2 features.
The following table shows example results for
the aforementioned BE classiﬁcation of 10000 chips of product D, using all 150
features or using just 2 or 3 speciﬁc ones.
#Features
#Cics
TP/FP
TN/FN
Accuracy%
Kappa
150
7
∞
36.4
97.4
0.638
3
3
∞
36.4
97.4
0.638
2
2
204
36.4
97.4
0.637
As can be seen in this table, if using all 150 columns, Algorithm 2 ﬁnds 7 Cics.
When we replaced this by 3 or even 2 very speciﬁc columns, the prediction
quality by Algorithm 6 was only marginally worse.
5.4.3
Addendum: Batch-Wise CutoﬀOptimization
If measurement data of several wafers of the same product type is put together
to some data lot, inhomogenities may occur, for example caused by parameter
shifts or by accumulation of defects in certain wafers. As a consequence, optimizing the cutoﬀover a complete data lot containing the measurement data
of—for example—some dozens of wafers, as Algorithm 5 does by default, may
be too coarse. In order to explore this further, we performed a series of experiments by not optimizing the cutoﬀonce over the whole series of wafers but by
repeating Algorithm 5 on a sequence of batches each containing a ﬁxed number
of chips instead.
In the rest of this subsection, results relate to predicting the BE overall defect
state ”BEpass” of product D (34550 chips) using 70%/5% of all positive/negative
26

===== Page 28 =====

Figure 9: SC(i) in BE classiﬁcation with product D, using only 2 features.
0
2000
4000
6000
8000
Object
0.00
0.25
0.50
0.75
1.00
1.25
1.50
1.75
2.00
NumSat [sampled]
Number of conditions satisfied per object
knowing only sampled distribution.
objects as samples for training, using 3 columns out of 150 as Cics as described
in subsection 5.4.2. The following table shows the results of some sequences of
runs of the classiﬁcation Algorithm 6 when performing the optimization step by
Algorithm 5 on a single batch instead of the complete lot. There is considerable
variation in the prediction quality for separate batches, as the following table
shows.
200
400
1000
1382
4000
Max
Kappa
1.0
0.920
0.729
0.669
0.619
Min
Kappa
0.0
0.196
0.433
0.423
0.535
Using a batch size of 1382 means wafer-wise cutoﬀoptimization. Looking at
how many batches could be classiﬁed with given kappa values, we obtained the
following numbers.
Batchsize 200
Batchsize 400
#Batches with κ = 1.0
13
0
#Batches with κ ∈[0.9, 1.0)
1
2
#Batches with κ ∈[0.8, 0.9)
11
2
#Batches with κ ∈[0.7, 0.8)
29
16
27

===== Page 29 =====

Figure 10: Cutoﬀoptimization of BE classiﬁcation with product D, using only
2 features.
0.0
0.5
1.0
1.5
2.0
Cutoff
0.2
0.4
0.6
0.8
1.0
Accuracy
0.0
0.5
1.0
1.5
2.0
Cutoff
0.0
0.1
0.2
0.3
0.4
0.5
0.6
Kappa
Cutoff vs Accuracy (left), cutoff vs Kappa (right) knowing only samples.
5.5
Classifying Iris Types
As a completely diﬀerent application in multiple respects—object count, feature
count and type of measurements—we tested Algorithm 6 with the classic iris
ﬂower data set [4] with the correction of [3]. It turns out that Algorithm 6
performs well on this data set despite of its small number of objects and features. The following results have all been obtained using nb = 5 or 6, 60%/1%
positive/negative training set size and specifying columns #3 and #4 as Cics.
The selection of Cics can also be done by prescribing b+ (see Algorithm 2), with
slightly worse quality of prediction.
Type
TP
FP
TN
FN
Kappa
Accuracy%
setosa
18
0
99
2
0.937
98.3
versicolor
13
1
98
7
0.727
93.3
virginica
13
0
99
7
0.756
94.1
In comparison, the classiﬁcation algorithm of [7] performed well only when classifying two of the three types.
5.6
Dimensional Reduction
As rendered by examples in subsections 5.4.2 and 5.3.1, there are applications
where not all features are necessary for chip classiﬁcations. Some features or
even groups of features in the baselying measurement data may be eminently
important whereas others may be dispensable without much degradation in
prediction quality. We will be using the notations of section 3 here.
Let Ha∗,nb : Qm →Qnb be the histogram function mapping m numbers to
the relative frequencies (h0, . . . , hnb−1) of these m numbers if distributing them
to nb equidistant intervals I0, . . . , Inb−1 with bounds a∗.
28

===== Page 30 =====

Deﬁnitions. Let j be some column index in {1, . . . , n}. Consider (h0, . . . , hnb−1) =
Ha∗,nb(x+,j) where x+,j = (xi,j : i ∈T +) ∈Q|T +| is the j-th column of Scale(X),
reduced to those rows representing positive training objects.
Let k∗be the
smallest index in {0, . . . , nb −1} with hk∗= max(h0, . . . , hnb−1). Then Ik∗is
the (left-most) interval of those occurring with maximum relative frequency.
Deﬁne
npos(j)
=
|{i ∈T + : xi,j ∈Ik∗}|
nneg(j)
=
|{i ∈T −: xi,j ∈Ik∗}|
ndiﬀ(j)
=
npos(j) −nneg(j)
As follows from the deﬁnition of Cic in subsection 3.1.3, some column j is
suited for being used as one Cic in computing the indicators by Algorithm 3
if the property of xi,j lying in the most-frequent interval of the j-th column’s
histogram can be used as a criterion separating as many as possible positive
objects (xi,1, . . . , xi,n) from negative objects. Since all the classiﬁcation algorithm knows in training is the samples, we must take histograms of the columns
reduced to the positive objects, (h0, . . . , hnb−1) = Ha∗,nb(x+,j) as deﬁned above.
So some column j is suited for being used as one Cic in computing the
indicators by Algorithm 3 if it has the following two properties.
- For many positive samples (xi,1, . . . , xi,n) (i ∈T +), xi,j lies in the mostfrequent interval Ik∗of Ha∗,nb(x+, j).
- For few negative samples (x¯ı,1, . . . , x¯ı,n) (¯ı ∈T −), x¯ı,j lies in the aforementioned Ik∗.
Therefore, those columns j that satisfy these two criteria to a high extent possess high ndiﬀ(j) = npos(j) −nneg(j) values which suggests using ndiﬀ(j) as a
relevance indicator of column j.
As an example, here is the top of some table listing column indices j ordered
by decreasing ndiﬀ(j):
Rank
j
ndiﬀ
npos
nneg
1
144
844
879
35
2
145
841
899
58
3
142
836
903
67
4
141
831
897
66
5
143
825
901
76
· · ·
· · ·
· · ·
· · ·
· · ·
146
1
0
24
24
147
5
-11
64
75
148
17
-112
271
383
149
16
-130
346
476
150
6
-230
765
995
29

===== Page 31 =====

This table has been extracted while classifying product D for the BE overall
defect state as described in 5.4.2.
As is clearly visible, the 3 columns j =
142, 143, 144 occur among the top ﬁve ranks. These 3 columns were those that
made it possible to use only 3 out of 150 columns as Cics with a negligible
degradation of prediction quality in FE prediction—see subsection 5.3.1 —and
in BE prediction—see subsection 5.4.2.
5.7
Automatizing The Finding of Cics
The ndiﬀranking detailed in section 5.6 can be used in order to try ﬁnding
relevant features, i.e., features that should be taken into respect when computing
the indicator values SC(i) by Algorithm 3. The resulting classiﬁcation algorithm
is the same as Algorithm 6 with the only diﬀerence being that the step
C := FindCics(Scale(X), T +, T −, b+, b−, nb) by Algorithm 2
is replaced by
C := AutoCics(Scale(X), T +, T −, nb, t) by Algorithm 7
which refers to the following algorithm. Let t be some number in {1, 2, . . . , n}
where n is the number of columns of the input data matrix X.
Input: Scale(X) ∈Qm×n, T + ⊆I+, T −⊆I−, nb ∈N≧3, t ∈{1, . . . , n}
Output: list L =

(js, a∗
ks, a∗
ks+1) | s = 1, . . . , t

1 for j = 1 . . . n do
2
set x+ = (xi,j | i ∈T +)
3
compute a∗= a∗
nb(x+)
4
compute ndiﬀ(j) as in section 5.6
5 end
6 R =
 j, ndiﬀ(j)

| j = 1, . . . , n; ordered by decreasing ndiﬀ(j)

7 set
 j1, ndiﬀ(j1)

, . . . ,
 jn, ndiﬀ(jn)

= R
8 return L =

(js, a∗
ks, a∗
ks+1) | s = 1, . . . , t

Algorithm 7: AutoCics(X, T +, T −, nb, t)
If t is set to some number in {1, . . . , n} then Algorithm 7 returns this many
columns with the highest ndiﬀvalues and thus can be plugged into the classiﬁcation algorithm 6 as an alternative for computing C by Algorithm 2.
5.7.1
Practical Results by Using AutoCics()
In order to get an impression of the practicability of the AutoCics() variant of
Algorithm 6, we tested the degree of stability of the column list L computed by
the AutoCics() Algorithm 7 against changing training set selection, training set
size and changing data lots of the same product type.
30

===== Page 32 =====

In order to compare the AutoCics() variant with results mentioned earlier
in this paper, we repeated the task as described in section 5.3.2: predict the S1
overall defect state of 10000 chips of Product B using minimal training sets of
only 2 positive and 1 negative samples. But unlike 5.3.2, we do not control the
selection of Cics by b+ but by the parameter t of the AutoCics() algorithm. b+
and b−are not used here.
The following table shows results for three diﬀerent settings of the parameter
t—called “#Top Cics Used” in this table—in Algorithm 7. When setting t = 50,
the diﬀerence in detection quality is small with Kappa changing from 0.983
to 0.980 and FP+FN increasing from 10+12 to 2+23. Because of these tiny
training sets, only 50 · 3 numbers of the matrix Scale(X) are used for classifying
10000 chips here.
#Top
FP
FN
Accuracy%
Kappa
Cics Used
50
2
23
99.7
0.980
45
81
23
99.0
0.921
40
277
23
97.0
0.799
5.7.2
Obsoleting Free Parameters by Using AutoCics()
Using AutoCics() by Algorithm 7 oﬀers the possibility to get rid of the necessity
of controlling the selection of Cics by the user. We can specify a default value
t∗for t, derived from n (= number of columns of X), which is to be used if the
user does not specify t himself. This makes it possible to do without specifying
b+, b−in Algorithm 6 as well as t in Algorithm 7.
We set t∗= ⌈0.1 · n⌉so that the top 10% ranks of the ndiﬀtable described
in sections 5.6 and 5.7 are used as indicator columns.
The following table shows results of the setting described in 5.7.1, but this
time t∗is used for t. As Product B contains 332 measurements per chip, t∗is
34 here. We use 5 chips for training, 3 positive samples and 2 negative ones.
TP
657
FP
13
TN
9301
FN
25
TP/(TP+FN)%
96.3
TN/(TN+FP)%
99.9
FP/(TP+FN)%
1.9
FN/(TN+FP)%
0.3
TP/FP
50.5
TN/FN
372.0
Accuracy%
99.6
Kappa
0.970
31

===== Page 33 =====

6
Algorithmic Complexity
In what follows, we count operations +, −, ×, ÷, ≦, ⌊·⌋and √· at unit cost
for all occurring numbers.
This is a realistic model when using ﬁxed-width
rational number types like 64-bit ﬂoating point on a real computer. By this,
we assume a somewhat idealized computer by ignoring certain limitations of
data types frequently used for unit-cost- (or constant-cost)-arithmetics on real
computer hardware: limited value range, rounding errors in representing rational numbers whose denominator is not a power of 2, approximations of √·
etc. Furthermore, we ignore costs associated with accessing and modifying simple data structures—mainly, accessing single numbers, vectors and matrices of
numbers in memory. Counting these may increase costs by polylogarithmic factors, depending on the chosen data structures, the input size and the computer
hardware the user is going to use.
6.1
Scaling (Algorithm 1)
For X ∈Qm×n we need to compute n means and n standard deviations in
O(m · n), and compute xi,j−µj
σj
for all i, j. Altogether, this takes
O(m · n)
arithmetic operations, including n square roots.
6.2
Computing one histogram Ha,nb(x1, . . . , x ¯m)
Let a = (−∞< a1 < · · · < anb−1 < ∞).
Case 1: The inner boundaries a1, . . . , anb−1 are chosen by the user: For
every i we ﬁnd the unique bin [ak, ak+1) which contains xi by binary search on
the boundaries vector a, which takes log(nb) comparisons plus incrementing one
of nb counters. So the overall complexity is in
O
 ¯m · log(nb) + nb

.
Case 2: The inner boundaries a1, . . . , anb−1 are ﬁxed like when using a∗: Let
x = (x1, . . . , x ¯m). For every i we can compute the unique bin [ak, ak+1) (k ≧1)
containing xi by ﬁrst computing the constant bin width w = max(x)−min(x)
nb−2
, then
k = 1 + ⌊xi−min(x)
w
⌋.6 Obviously, k ∈{1, . . . , nb −1} and xi ∈[ak, ak+1). The
left-most interval [a0, a1) = [−∞, a1) stays empty. We need to compute min(x)
and max(x) only once and have O(1) arithmetic operations plus incrementing
one of nb counters for every xi which sums up to
O( ¯m + nb)
operations altogether.
Note that in our applications ¯m is |T +| or |T −| which are both ≦m (= row
number of X).
6Note that the right-most inner interval must be [anb−2, anb−1+ε) instead of [anb−2, anb−1)
for anb−1 = max(x1, . . . , x ¯
m) for formal reasons, see subsection 3.1.2.
32

===== Page 34 =====

6.3
Computing FindCics(Scale(X), T +, T −, b+, b−, nb) (Algorithm
2)
For every column index j = 1, . . . , n: Computing min(x+) and max(x+) takes
O(|T +|) operations, thus computing a∗is in O(|T +| + nb).
Computing the
histograms h+ and h−takes O(|T +|) + O(|T −|) + O(nb) operations according
to Case 2 of above. Computing h+
max and k+ takes nb comparisons at most.
The output consists of at most 3 · n numbers.
So computing FindCics(Scale(X), T +, T −, b+, b−, nb) from Scale(X) takes at
most
O
 n · (|T +| + |T −| + nb)

operations.
Remark. This is in O
 n · (m + nb)

since |T +| + |T −| ≦m.
Theorem.
Computing C = FindCics(Scale(X), T +, T −, b+, b−, nb) from
X ∈Qm×n takes at most
O
 n · (m + nb)

operations.
Proof. This follows directly from the complexity of computing Scale(X),
then computing C = FindCics(Scale(X), T +, T −, b+, b−, nb) from Scale(X) and
from the above Remark, qed.
6.4
Computing SC(I¬T) (Algorithm 3)
Given C = FindCics(Scale(X), T +, T −, b+, b−, nb), for each one of I¬T nontraining objects we must perform at most |C| interval containment decisions
which sums up to
|C| · O(|I¬T |)
operations.
6.5
Computing predictions F(i, c) (Algorithm 4)
Given SC(I¬T ) and c, this sums up to
O(|I¬T |)
operations for all i ∈I¬T .
6.6
Na¨ıve cutoﬀselection
Given I+
¬T , I−
¬T and SC(I¬T ) from Algorithm 3, Computing Av0, Av1 takes
O(|I+
¬T | + |I−
¬T |) ⊆O(|I¬T )
operations.
33

===== Page 35 =====

6.7
Optimizing cutoﬀc (Algorithm 5)
Let Q : Br×Br −→R be a function measuring some type of similarity of two 0-1
vectors and let be E(r) the maximum number of operations it takes to evaluate
Q(v, w) for any v, w ∈Br. Given C = FindCics(Scale(X), T +, T −, b+, b−, nb)
from Algorithm 2, Algorithm 5 repeats
 Smax
C
−Smin
C
+ 1

times Algorithm 4 for
obtaining a prediction vector F(c) using O(|I¬T |) operations each, and evaluates
Q(·, ·) on two 0-1 vectors of length r = |I¬T | using at most E(|I¬T |) operations
each. From Smin
C
≧0 and Smax
C
≦|C| follows Smax
C
−Smin
C
+1 ≦|C|+1. Computing
Qopt and Copt both take O(Smax
C
−Smin
C
+ 1) as c takes just this many values in
the for-loop.
So the total number of operations Algorithm 5 takes is limited by
(|C| + 1) · O (|I¬T | + E(|I¬T |))
⊆O
 |C| ·
 |I¬T | + E(|I¬T |)

6.8
Overall complexity
Adding the upper bounds for Scale(X), for computing FindCics
 Scale(X), . . .

and for computing SC(I¬T ) gives an operation count of
O
 n · m + n · (m + nb) + |C| · |I¬T |

⊆
O
 n · (m + nb + |I¬T |)

(since |C| ≦n)
⊆
O
 n · (m + nb)

(since |I¬T | ≦m)
If we add steps for cutoﬀselection or optimization and use |I¬T | ≦m twice we
get:
O
 n · (m + nb)

with na¨ıve cutoﬀselection
O

n ·
 m + nb + E(|I¬T |)

when optimizing cutoﬀc
Using |I¬T | ≦m once again, we get:
Theorem. Given X ∈Qm×n, v ∈Bm, training sets T +, T −⊆{1, . . . , m},
b+, b−∈Q, nb ∈N≧3, the overall number of operations it takes to
• compute Scale(X)
• compute C = FindCics
 Scale(X), T +, T −, b+, b−, nb

• compute SC(I¬T )
 where I¬T = {1, . . . , m} \ (T + ∪T −)

• select cutoﬀc
• compute predictions F(i, c) from this by Algorithm 4
is limited by
O
 n · (m + nb)

with na¨ıve cutoﬀselection
O

n ·
 m + nb + E(m)

when optimizing cutoﬀc
34

===== Page 36 =====

where E(m) is the number of operations it takes to evaluate the chosen similarity
measure Q : Bm × Bm −→R.
Note. The direct input of Algorithms 3 and 5 is C, not X or Scale(X), so
their complexity is actually linear in the number of Cics |C|. In the worst case
|C| equals n but in practical settings |C| is frequently much smaller than n, for
example 20 versus 150. So Algorithms 3 and 5 become the faster the smaller
the number of Cics |C| is. Indeed, the implementation on a real computer shows
that |C| is an important determinant for time consumption.
35

===== Page 37 =====

7
Uniting Two Classiﬁcation Algorithms
There are several ways to combine multiple raters with the goal of obtaining a
better rater. Some well-known ways are making majority decisions, or making
a decision according to the number of the “1” (or “0”) of the single raters
exceeding a certain threshold or not. What we are doing in this section instead
is combining the baselying principles of two classiﬁcation algorithms—Algorithm
6 and the one of [7]—into one algorithm.
In this section, we will use the same notations and designations as in section
3
Deﬁnition.
We will say that a Cic j is active for object i represented
by xi,1, . . . , xi,n if in Algorithm 3, xi,j ∈

a∗
k+, a∗
k++1

. Here, i ∈{1, . . . , m},
j ∈{1, . . . , n}.
In SC(i) computed by Algorithm 3, the number of active Cics for object i is
used as an indicator for positivity of object i. Actually, there is no reason for
just counting and not trying to go further. In order to be more speciﬁc, one
could replace SC(i) by the bit pattern induced by the active Cics for object i.
Deﬁnition. Let
AC(i) : {1, . . . , m} →B|C|
i 7→ai
where
ai(j) =
(
1
if Cic j is active for object i
0
else
Now let be U + ⊆I+ some set of positive objects we will use as a third set of
training indices along with T +, T −. We need to reduce I¬T deﬁned in subsection
3.1.2 to
I¬T,¬U = {1, . . . , m} \
 T + ∪T −∪U +
in order to skip the objects of three training sets in the classiﬁcation loop.
Let be ⟨a, b⟩= Pr
k=1 ak · bk the inner product of two vectors of size r, and
H(a) = Pr
k=1 ak the Hamming weight of a ∈Br.
Deﬁnition. Let be i ∈{1, . . . , m} and U + as above.
qmax(i, U +)
:=
max
j∈U +
⟨AC(i), AC(j)⟩
H (AC(i))
qmin(i, U +)
:=
min
j∈U +
⟨AC(i), AC(j)⟩
H (AC(i))
Remark. qmax(i, U +) is in strict analogy to what is denoted by s(x, T) in
subsection “Dense Formulation” of section “The Algorithms” in [7], the diﬀerence being that in [7] we operate on bit patterns
 P1(x(i)), . . . , Pn(x(i))

induced
by component-wise thresholding x(i) = i-th line of the scaled input matrix, whereas
by computing qmax, qmin here we operate on the Cic activity pattern of line i of
the (scaled) input matrix.
36

===== Page 38 =====

Deﬁnition. As in subsection 3.1.4, let C = FindCics(Scale(X), T +, T −, b+, b−, nb)
be the output of Algorithm 2. Deﬁne two alternative indicators by:
Qmax
C,U + : I¬T,¬U →[0, 1] ⊆Q
i 7→qmax(i, U +)
Qmin
C,U + : I¬T,¬U →[0, 1] ⊆Q
i 7→qmin(i, U +)
The following algorithm is formulated in a way that it can be used as a
drop-in replacement for Algorithm 3 in subsection 3.1.4.
Input: C = FindCics(Scale(X), T +, T −, U +, b+, b−, nb)
Output: Qmax
C,U + (I¬T,¬U) , Qmin
C,U + (I¬T,¬U) ∈Q|I¬T,¬U|
1 for i ∈I¬T,¬U do
2
compute Qmax
C,U +(i)
3
compute Qmin
C,U +(i)
4 end
Algorithm 8: Computing alternative indicators Qmax
C,U +(i), Qmin
C,U +(i)
We can now formulate a variant of the classiﬁcation Algorithm 6 of subsection 3.1.7 which uses Qmax
C,U + (I¬T,¬U) , Qmin
C,U + (I¬T,¬U) as indicators instead of
SC (I¬T ).
Input: X ∈Qm×n; b+, b−∈Q; nb ∈N≧3; t+, t−, u+ ∈N≧1
Output: F(Copt) ∈B|I¬T,¬U|
1 Scale(X) by Algorithm 1
2 select random subsets T + ⊆I+ of size t+, T −⊆I−of size t−, U + ⊆I+
of size u+
3 C := FindCics(Scale(X), T +, T −, b+, b−, nb) by Algorithm 2
4 compute Qmax
C,U + (I¬T,¬U), Qmin
C,U + (I¬T,¬U) by Algorithm 8
5 compute the optimum cutoﬀCopt and F(Copt) by an adaption of
Algorithm 5 and Algorithm 4
Algorithm 9: Classiﬁcation algorithm
By “an adaption of Algorithm 5 and Algorithm 4” we mean the following two
modiﬁcations:
−In Algorithm 5, replacing the looping c = Smin
C
, Smin
C
+ 1, . . . , Smax
C
by
looping over some grid C ⊂[0, 1] ⊂Q or by the method of subsection
“Reﬁned Method For CutoﬀSelection” in [7].
37

===== Page 39 =====

−In Algorithm 4 (called by Algorithm 5), replacing the computation of SC(i)
by Qmax
C,U +(i), Qmin
C,U +(i) as deﬁned above.
The potential of Algorithm 9 has yet to be explored and will be a subject of
future research.
38

===== Page 40 =====

8
Conclusion
We have presented a histogram-based algorithm for object classiﬁcation in subsection 3.1.7 together with some worst-case complexity analysis in section 6,
and rendered the detection and prediction of frontend and backend defects in
semiconductor chips based on wafer measurement data as main application by
discussing classiﬁcation results of data from real-world wafer fabrication in section 5. Furthermore, we laid out a method of searching for candidate indicator
columns in subsection 5.6 which enables the feature count to be reduced dramatically in some applications. We presented an algorithm using ndiﬀas a feature
relevance indicator in section 5.7.
Finally, we presented an algorithm that unites some baselying principle of the
classiﬁcation algorithm of [7] with the principles of the classiﬁcation Algorithm
6 as detailed in this paper.
In subsection 5.4 of this paper, we have started tackling the problem of
predicting the backend overall defect states of semiconductor chips which has
been mentioned in the last paragraph of [7] as one topic of ongoing research.
The results which we report on in section 5 conﬁrm that predicting the backend
overall defect state for the type of data our classiﬁcations are performed on using
only frontend measurements seems to be harder of a machine learning problem
than predicting the frontend overall defect state.
39

===== Page 41 =====

List of Figures
1
SC(i) in the S2 classiﬁcation with product C.
. . . . . . . . . . .
18
2
Cutoﬀoptimization of the S2 classiﬁcation with product C. . . .
19
3
Cutoﬀoptimization of the S1 classiﬁcation with product B. . . .
21
4
Histogram collection of 5 features, knowing all objects. . . . . . .
22
5
Histogram collection of 5 features, knowing 20%/10% sample sets
only. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
23
6
Histogram collection of 5 features, knowing 2+2 sample objects
only. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
24
7
SC(i) in BE classiﬁcation with product D, using all features. . . .
25
8
Cutoﬀoptimization of BE classiﬁcation with product D, using all
features. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
26
9
SC(i) in BE classiﬁcation with product D, using only 2 features. .
27
10
Cutoﬀoptimization of BE classiﬁcation with product D, using
only 2 features. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
28
References
[1] Richard M. Brugger. A Note on Unbiased Estimation of the Standard Deviation. The American Statistician, 23(4):32, 1969.
[2] Jacob Cohen. A coeﬃcient of agreement for nominal scales. Educational and
Psychological Measurement, 20(1):37–46, 1960.
[3] Dheeru Dua and Casey Graﬀ. UCI machine learning repository, 2017.
[4] Ronald Aylmer Fisher.
The use of multiple measurements in taxonomic
problems. Annals of Eugenics, 7(2):179–188, 1936.
[5] Charles R. Harris, K. Jarrod Millman, St´efan J. van der Walt, Ralf
Gommers, Pauli Virtanen, David Cournapeau, Eric Wieser, Julian Taylor, Sebastian Berg, Nathaniel J. Smith, Robert Kern, Matti Picus,
Stephan Hoyer, Marten H. van Kerkwijk, Matthew Brett, Allan Haldane,
Jaime Fern´andez del R´ıo, Mark Wiebe, Pearu Peterson, Pierre G´erardMarchant, Kevin Sheppard, Tyler Reddy, Warren Weckesser, Hameer Abbasi, Christoph Gohlke, and Travis E. Oliphant. Array programming with
NumPy. Nature, 585(7825):357–362, 2020.
[6] J. D. Hunter. Matplotlib: A 2D graphics environment. Computing in Science
& Engineering, 9(3):90–95, 2007.
[7] Thomas Olschewski. Fast Accurate Defect Detection in Wafer Fabrication.
https://arxiv.org/abs/2108.11757v1, 2021.
[8] D. E. Rumelhart, G. E. Hinton, and R. J. Williams. Learning representations
by back-propagating errors. Nature, 323(6088):533–536, 1986.
40
